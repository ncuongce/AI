{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import *\n",
    "from StringIO import StringIO\n",
    "from operator import *\n",
    "import copy\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_world( filename):\n",
    "    with open( filename, 'r') as f:\n",
    "        world_data = [x for x in f.readlines()]\n",
    "    f.closed\n",
    "    world = []\n",
    "    for line in world_data:\n",
    "        line = line.strip()\n",
    "        if line == \"\": continue\n",
    "        world.append([x for x in line])\n",
    "    return world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dict of movement costs. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*': -3, '.': -1, '^': -5, '~': -7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not reference this as a global variable.\n",
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for NEIGHBORS. You'll need to work this into your actions, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEIGHBORS = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function argmax is used to find the action that have maximum expected value for a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def argmax(actions, func):\n",
    "    max_action = actions[0]; max_value = func(max_action)\n",
    "    for a in actions:\n",
    "        value = func(a)\n",
    "        if value > max_value:\n",
    "            max_action, max_value = a, value\n",
    "    return max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function get_best_policy is used to get the best policy for an action at a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_policy(states, actions, q):\n",
    "    pi = {}\n",
    "    for s in states:\n",
    "        pi[s] = argmax(actions, lambda a:get_utility(s, a, q))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_utility(s, a, q):\n",
    "    return q[(s,a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q_learning function uses Q-Learning (reinforcement learning) to train the agent. It returns policies (the agent should move left, right, up, or down at diffirent states) after learning the world which is a grid map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_learning(world, costs, goal, reward, actions, gamma, alpha, max_episode):\n",
    "    states = set()\n",
    "    rewards = {}\n",
    "    for x in range(len(world[0])):\n",
    "        for y in range(len(world)):\n",
    "            if world[y][x] in costs:\n",
    "                rewards[x, y] = costs[world[y][x]]\n",
    "                states.add((x,y))\n",
    "            else:\n",
    "                rewards[x, y] = None\n",
    "    rewards[goal] = reward\n",
    "    def get_state(state, action):\n",
    "        state1 = tuple(map(add, state, action))\n",
    "        if (state1 in states):\n",
    "            return state1#move to new state\n",
    "        else:\n",
    "            return state#bound back\n",
    "    def get_actions(state):\n",
    "        if state == goal:\n",
    "            return [None]\n",
    "        else:\n",
    "            return actions\n",
    "    def get_greedy_action(state):\n",
    "        rand = random.choice([0,1,2,3,4,5,6,7,8,9])#10% greedy\n",
    "        if rand == 0:\n",
    "            return argmax(actions, lambda a:get_utility(state, a, q))\n",
    "        else:\n",
    "            return random.choice(actions)\n",
    "    def get_init_state(goal):\n",
    "        for a in actions:\n",
    "            state1 = tuple(map(add, goal, a))\n",
    "            if state1 in states:\n",
    "                return state1\n",
    "        return (0, 0)\n",
    "    checkpoint = get_init_state(goal)\n",
    "    def update_checkpoint(checkpoint, state):\n",
    "        dx1 = abs(state[0]-goal[0])\n",
    "        dy1 = abs(state[1]-goal[1])\n",
    "        d1 = dx1+dy1\n",
    "        dx2 = abs(checkpoint[0]-goal[0])\n",
    "        dy2 = abs(checkpoint[1]-goal[1])\n",
    "        d2 = dx2+dy2\n",
    "        if (d1 > d2):\n",
    "            checkpoint = state\n",
    "    q = {}\n",
    "    for s in states:\n",
    "        for a in actions:\n",
    "            q[(s,a)] = 0\n",
    "    for e in range(max_episode):\n",
    "        s = checkpoint\n",
    "        while (s != goal):\n",
    "            a = get_greedy_action(s)#explore, can also use exploit greedy\n",
    "            s1 = get_state(s, a)\n",
    "            r = rewards[s1]\n",
    "            q[(s,a)] = (1-alpha) * q[(s,a)] + alpha * (r + gamma * max([q[(s1,a1)] for a1 in get_actions(s)]))\n",
    "            s = s1\n",
    "            update_checkpoint(checkpoint, s)\n",
    "    return get_best_policy(states, actions, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print_policy prints out the 2d policy map with steps up '^', down 'v', left '<', and right '>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_policy(world, policy):\n",
    "    moves = {(0,-1):'^', (1,0):'>', (0,1):'v', (-1,0):'<'}\n",
    "    for y in range(len(world)):\n",
    "        str = \"\"\n",
    "        for x in range(len(world[0])):\n",
    "            if (x, y) in policy:\n",
    "                str += moves[policy[(x,y)]]\n",
    "            else:\n",
    "                str += 'x'\n",
    "        print str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_world(world):\n",
    "    for y in range(len(world)):\n",
    "        str = \"\"\n",
    "        for x in range(len(world[0])):\n",
    "            str += world[y][x]\n",
    "        print str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function value_iteration uses value iteration to solve the MDP grid map. It returns V and Pi, V is the largest expected discounted reward for particular states. It is used as a reference for Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "world1 = read_world(\"world1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = q_learning(world1, costs, (3,0), 100, NEIGHBORS, 0.9, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....\n",
      ".x..\n",
      "....\n"
     ]
    }
   ],
   "source": [
    "print_world(world1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>^\n",
      "vx>^\n",
      ">>>^\n"
     ]
    }
   ],
   "source": [
    "print_policy(world1, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world2 = read_world(\"world2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = q_learning(world2, costs, (6,6), 100, NEIGHBORS, 0.9, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".******\n",
      ".******\n",
      ".******\n",
      ".......\n",
      "******.\n",
      "******.\n",
      "******.\n"
     ]
    }
   ],
   "source": [
    "print_world(world2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^<<<^vv\n",
      "<<vvvvv\n",
      "^<vv>vv\n",
      "^>>>>>v\n",
      "v^>>>>v\n",
      ">>>>>>v\n",
      "v>>>>>^\n"
     ]
    }
   ],
   "source": [
    "print_policy(world2, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world3 = read_world(\"world3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....**********............\n",
      ".......*********..xxxxxxx..\n",
      "....xx***********xxx^^^xx^^\n",
      "....^xxx****~~*****..^^xx^.\n",
      "...^^xx**..~~~~***...^xxx^.\n",
      ".^^^xx^^....~~~~~.....^x^..\n",
      ".^^xx^^....^xxx~~~.....^...\n",
      "..^^^^^......^xxx~~~..^^^..\n",
      "...^^^......^^xx.~~..^^^...\n",
      "...~~~..^^^xxxx...~.^^^....\n",
      "..~~~~~.^^xxx^.....^xxx^...\n",
      ".~~~~~..^xx^....~~..^xx^...\n",
      "~~~~~..^^xx^.~~~~...^x^....\n",
      ".~~~~..^**^....~~~~..^.....\n",
      "....x..****^^^^.~~~..^x^...\n",
      "...xxx******xxx^^.~.^xx^...\n",
      "..xx**********xxx..xxx.....\n",
      "...xx***********xxxx.......\n",
      "...xxx********...^^........\n",
      "....xxx******..........~~~~\n",
      "..^^^^xx*****.x.....~~~~~~~\n",
      "....^^^xxx**xx......~~~~~~~\n",
      "......^^^xxxx....^^..~~~~~~\n",
      ".^^..^^^^^.....^^xx^^.~~~~~\n",
      "^x^^^^.....xxx^^xx.xx^^~~~~\n",
      "^xxx^.....^^xxxx^^^^xxx~~~~\n",
      "^^..........^^^^^....^^^...\n"
     ]
    }
   ],
   "source": [
    "print_world(world3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = q_learning(world3, costs, (26,26), 100, NEIGHBORS, 0.9, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^<<<vvv<<^v^<v>^>v>v^^v>>^^\n",
      "^v<v>><<^>v<<<^^><xxxxxxx^^\n",
      "v>><xx<<^>^v>^<>^xxxvvvxx^^\n",
      ">^<^<xxxvvv<<>>v<>vvv<<xx>v\n",
      ">^^<^xxvvvvv^>^><>><<<xxx>v\n",
      "<^^^xx^v><><<v<^>>^<^v<x>v<\n",
      "vv>xx<>v>v<vxxx^^^^>^^<>>><\n",
      ">vv>^<>>>v>><<xxx<^^^^<^>^<\n",
      "^vv<>>>^<^>^<^xxv<>>^<<vv^<\n",
      "v^<<^>>^^^^xxxx<vv>^<^>><^v\n",
      "^<<<v>v^<^xxx>^^^^<<xxx^v^v\n",
      "^^^vv>v<<xx>v>>^<^^v<xx>v>>\n",
      "^^vv>vv^vxxvvvv^>><v<x>>^vv\n",
      "<vvv>v<<>^>><<<<>^^^<<>^>>^\n",
      "^v>>x<<<vvv^^^^<<v>^<<x>>v<\n",
      "<v^xxx^^v<^<xxx^>v>v<xxvv^^\n",
      "><xx^v>^v<v>vvxxx>>xxxvv<>^\n",
      "<^<xx<v>v^<^^vvvxxxx>><<^>v\n",
      "><^xxxvv<><<v>>vvvvv>>v^^>>\n",
      "v<^<xxx<v<<>>><<vvv^^<<<^^^\n",
      "<^vvvvxx>^^v>vx^^^<<^^^<^^^\n",
      "><v<vvvxxx>>xxv^^^<^v<^v><v\n",
      "<>><v<<<vxxxxv>><<^^<<v<<^v\n",
      "^^^^^<vvvvv^<^^<^xx^^<<^vvv\n",
      "^x^^^>>vv>>xxx^>xx<xx^<>v>v\n",
      "^xxxv>vvv^vvxxxxvvvvxxx>>>v\n",
      "<>^>v>v>v<<<<<^>>>>>v<>>>>^\n"
     ]
    }
   ],
   "source": [
    "print_policy(world3, policy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
